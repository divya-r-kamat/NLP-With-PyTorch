# Sequence to Sequence Modeling 

Train model1 based on [Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](https://arxiv.org/abs/1406.1078) and model2 based on [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473) on  following 4 datasets from https://kili-technology.com/blog/chatbot-training-datasets/

1.  [Question-and-answer dataset](http://www.cs.cmu.edu/~ark/QA-data/): This corpus includes Wikipedia articles, factual questions manually generated from them, and answers to these manually generated questions for use in academic research.
    [Link to Code](Question_Answer_Seq_2_Seq)

2. [Customer Support on Twitter](https://www.kaggle.com/thoughtvector/customer-support-on-twitter) : This Kaggle dataset includes more than 3 million tweets and responses from leading brands on Twitter.
    [Link to Code](Twitter_Seq_2_Seq)

3. [The Stanford Question Answering Dataset (SQuAD)](https://rajpurkar.github.io/SQuAD-explorer/) is a set of reading comprehension data consisting of questions asked by social workers on a set of Wikipedia articles, where the answer to each question is a segment of text, or span, of the corresponding reading passage. With more than 100,000 question-answer pairs on more than 500 articles, SQuAD is significantly larger than previous reading comprehension datasets. SQuAD2.0 combines the 100,000 questions from SQuAD1.1 with more than 50,000 new unanswered questions written in a contradictory manner by crowd workers to look like answered questions.
    [Link to Code](SQuAD_Seq_2_Seq)

4. [CoQA](https://stanfordnlp.github.io/coqa/) is a large-scale data set for the construction of conversational question answering systems. The CoQA contains 127,000 questions with answers, obtained from 8,000 conversations involving text passages from seven different domains.
    [Link to Code](CoQA_Seq_2_Seq)
