Question/Answer Dataset, Release 1.2
====================================

This is the README file for the Question/Answer dataset generated by students 
who took undergraduate natural language processing courses taught by Noah Smith 
at Carnegie Mellon and Rebecca Hwa at the University of Pittsburgh during 
Spring 2008, Spring 2009, and Spring 2010.

There are three directories, one for each year of students: S08, S09, and S10.

The file "question_answer_pairs.txt" contains the questions and answers. The first line of the file contains 
column names for the tab-separated data fields in the file. This first line follows:

ArticleTitle    Question        Answer  DifficultyFromQuestioner        DifficultyFromAnswerer  ArticleFile

Field 1 is the name of the Wikipedia article from which questions and answers initially came.
Field 2 is the question.
Field 3 is the answer.
Field 4 is the prescribed difficulty rating for the question as given to the question-writer. 
Field 5 is a difficulty rating assigned by the individual who evaluated and answered the question, 
which may differ from the difficulty in field 4.
Field 6 is the relative path to the prefix of the article files. html files (.htm) and cleaned 
text (.txt) files are provided.

Questions that were judged to be poor were discarded from this data set.

There are frequently multiple lines with the same question, which appear if those questions were answered 
by multiple individuals. 

This particular release was prepared by Kevin Gimpel, but the data collection process 
was performed by Noah Smith, Mike Heilman, Rebecca Hwa, Shay Cohen, and many CMU students 
and Pitt students.

License Information:

The contents of S08 and S09 are released under the GFDL (http://www.gnu.org/licenses/fdl.html) and the contents of S10 are released under the CC BY-SA 3.0 (http://creativecommons.org/licenses/by-sa/3.0/). A copy of the GFDL license is included in the file named LICENSE-S08,S09. The reason for the different licenses is because Wikipedia moved from the GFDL to the CC BY-SA 3.0 license in the summer of 2009..

Release history:

2/18/2010: Version 1.0
  * Partial set of question/answer pairs from 2008 and 2009

8/6/2010: Version 1.1
  * All question/answer pairs from 2008, 2009, and 2010

8/23/2013: Version 1.2
  * Same data as Version 1.1, but now released under standard licenses

If you use this data in a publication, please cite the following:

Noah A. Smith, Michael Heilman, and Rebecca Hwa
Question Generation as a Competitive Undergraduate Course Project
In Proceedings of the NSF Workshop on the Question Generation Shared Task and Evaluation Challenge, Arlington, VA, September 2008. 
Available at: http://www.cs.cmu.edu/~nasmith/papers/smith+heilman+hwa.nsf08.pdf

Kevin Gimpel
kgimpel@cs.cmu.edu
kgimpel@ttic.edu
8/23/2013


# [Model1](Question_Answer_Dataset_Learning_Phrase_Representation_RNN_Encoder_Decoder.ipynb)

Model Architecture

    Seq2Seq(
      (encoder): Encoder(
        (embedding): Embedding(2197, 256)
        (rnn): GRU(256, 512)
        (dropout): Dropout(p=0.5, inplace=False)
      )
      (decoder): Decoder(
        (embedding): Embedding(1504, 256)
        (rnn): GRU(768, 512)
        (fc_out): Linear(in_features=1280, out_features=1504, bias=True)
        (dropout): Dropout(p=0.5, inplace=False)
      )
    )

Training Log

    Epoch: 01 | Time: 0m 4s
     Train Loss: 5.185 | Train PPL: 178.506
      Val. Loss: 3.947 |  Val. PPL:  51.773
    Epoch: 02 | Time: 0m 3s
     Train Loss: 4.433 | Train PPL:  84.210
      Val. Loss: 3.851 |  Val. PPL:  47.055
    Epoch: 03 | Time: 0m 4s
     Train Loss: 4.311 | Train PPL:  74.530
      Val. Loss: 3.891 |  Val. PPL:  48.950
    Epoch: 04 | Time: 0m 4s
     Train Loss: 4.208 | Train PPL:  67.226
      Val. Loss: 3.845 |  Val. PPL:  46.747
    Epoch: 05 | Time: 0m 3s
     Train Loss: 4.011 | Train PPL:  55.228
      Val. Loss: 3.757 |  Val. PPL:  42.821
    Epoch: 06 | Time: 0m 3s
     Train Loss: 3.896 | Train PPL:  49.208
      Val. Loss: 3.723 |  Val. PPL:  41.389
    Epoch: 07 | Time: 0m 3s
     Train Loss: 3.809 | Train PPL:  45.121
      Val. Loss: 3.712 |  Val. PPL:  40.943
    Epoch: 08 | Time: 0m 3s
     Train Loss: 3.696 | Train PPL:  40.277
      Val. Loss: 3.727 |  Val. PPL:  41.544
    Epoch: 09 | Time: 0m 3s
     Train Loss: 3.574 | Train PPL:  35.648
      Val. Loss: 3.807 |  Val. PPL:  45.033
    Epoch: 10 | Time: 0m 3s
     Train Loss: 3.503 | Train PPL:  33.224
      Val. Loss: 3.894 |  Val. PPL:  49.101

Test Loss

    | Test Loss: 3.558 | Test PPL:  35.110 |
       
    
# [Model2](Question_Answer_Dataset_Sequence_to_Sequence_using_Attention.ipynb)

Model Architecture

    Seq2Seq(
      (encoder): Encoder(
        (embedding): Embedding(2197, 256)
        (rnn): GRU(256, 512, bidirectional=True)
        (fc): Linear(in_features=1024, out_features=512, bias=True)
        (dropout): Dropout(p=0.5, inplace=False)
      )
      (decoder): Decoder(
        (attention): Attention(
          (attn): Linear(in_features=1536, out_features=512, bias=True)
          (v): Linear(in_features=512, out_features=1, bias=False)
        )
        (embedding): Embedding(1504, 256)
        (rnn): GRU(1280, 512)
        (fc_out): Linear(in_features=1792, out_features=1504, bias=True)
        (dropout): Dropout(p=0.5, inplace=False)
      )
    )

Training Log

    Epoch: 01 | Time: 0m 8s
     Train Loss: 4.988 | Train PPL: 146.642
      Val. Loss: 3.802 |  Val. PPL:  44.788
    Epoch: 02 | Time: 0m 8s
     Train Loss: 4.286 | Train PPL:  72.709
      Val. Loss: 3.696 |  Val. PPL:  40.294
    Epoch: 03 | Time: 0m 8s
     Train Loss: 4.115 | Train PPL:  61.235
      Val. Loss: 3.700 |  Val. PPL:  40.465
    Epoch: 04 | Time: 0m 8s
     Train Loss: 3.936 | Train PPL:  51.189
      Val. Loss: 3.618 |  Val. PPL:  37.270
    Epoch: 05 | Time: 0m 8s
     Train Loss: 3.675 | Train PPL:  39.443
      Val. Loss: 3.593 |  Val. PPL:  36.357
    Epoch: 06 | Time: 0m 8s
     Train Loss: 3.395 | Train PPL:  29.824
      Val. Loss: 3.558 |  Val. PPL:  35.098
    Epoch: 07 | Time: 0m 8s
     Train Loss: 3.094 | Train PPL:  22.058
      Val. Loss: 3.543 |  Val. PPL:  34.575
    Epoch: 08 | Time: 0m 8s
     Train Loss: 2.770 | Train PPL:  15.961
      Val. Loss: 3.538 |  Val. PPL:  34.403
    Epoch: 09 | Time: 0m 8s
     Train Loss: 2.453 | Train PPL:  11.628
      Val. Loss: 3.541 |  Val. PPL:  34.486
    Epoch: 10 | Time: 0m 8s
     Train Loss: 2.214 | Train PPL:   9.152
      Val. Loss: 3.581 |  Val. PPL:  35.895

Test Loss

    | Test Loss: 3.439 | Test PPL:  31.145 |
